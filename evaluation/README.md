# Evaluatuin
### Retrieval Evaluation

For retrieval evaluation, a ground truth dataset was generated by creating variations of the original questions stored in the vector database. Using an LLM, five different questions were crafted for each existing question-answer pair, forming a new CSV file. This approach ensured a broad testing base to evaluate the effectiveness of retrieval.

The FAISS vector database was then evaluated on this new dataset by running retrieval queries on the newly created questions. The performance of the retrieval system was measured using two key metrics:
- **Hit Rate**: 0.95
- **MRR (Mean Reciprocal Rank)**: 0.764

The evaluation process was executed in the notebook `evaluation/evaluate_retrival.ipynb`, and the results show that the retrieval system consistently returned relevant results for the provided queries.

### RAG Evaluation

The RAG evaluation focused on assessing how well the generated responses matched the original answers in the vector database. For each question in the vector DB, both the original reference answer and the chatbotâ€™s LLM-generated answer (using the RAG flow) were compared.

Two methods were used for evaluation:
1. **Cosine Similarity**: The cosine similarity between the original and generated answers was computed, yielding a mean of **0.8425**, indicating a high level of similarity between the responses.
2. **LLM-Based Judgments**: A secondary LLM was used to judge the relevance of the answers. Out of 17 responses, 12 were labeled as "RELEVANT," and 5 were labeled as "PARTLY RELEVANT."

